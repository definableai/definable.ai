---
title: Quickstart
description: Get up and running with Definable in under 5 minutes.
icon: bolt
---

## Install

<CodeGroup>
```bash pip
pip install definable
```

```bash uv
uv add definable
```
</CodeGroup>

## Set Your API Key

```bash
export OPENAI_API_KEY="sk-..."
```

## Your First Model Call

Call an LLM with a few lines of code:

```python
from definable.model.openai import OpenAIChat
from definable.model.message import Message

model = OpenAIChat(id="gpt-4o")
response = model.invoke(
    messages=[Message(role="user", content="Hello!")],
    assistant_message=Message(role="assistant", content=""),
)
print(response.content)
```

## Your First Agent

Create an agent with tools that can take actions:

```python
from definable.model import OpenAIChat
from definable.tool.decorator import tool
from definable.agent import Agent

@tool
def get_weather(city: str) -> str:
    """Get the current weather for a city."""
    return f"The weather in {city} is sunny, 72°F."

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[get_weather],
    instructions="You are a helpful weather assistant.",
)

output = agent.run("What's the weather in San Francisco?")
print(output.content)
```

## Your First RAG Query

Build a knowledge base and search it:

```python
from definable.embedder import OpenAIEmbedder
from definable.knowledge import Knowledge
from definable.vectordb import InMemoryVectorDB

knowledge = Knowledge(
    vector_db=InMemoryVectorDB(),
    embedder=OpenAIEmbedder(),
)

# Add documents
knowledge.add("Definable is a Python framework for building AI agents.")
knowledge.add("It supports OpenAI, DeepSeek, Moonshot, and xAI models.")

# Search
results = knowledge.search("What models does Definable support?")
for doc in results:
    print(doc.content)
```

## Your First Memory-Enabled Agent

Give an agent persistent memory that works across conversations:

```python
from definable.agent import Agent
from definable.memory import Memory, SQLiteStore
from definable.model import OpenAIChat

memory = Memory(store=SQLiteStore("./memory.db"))

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    instructions="You are a helpful assistant with persistent memory.",
    memory=memory,
)

# The agent remembers this across conversations
output = agent.run("My name is Alice and I work at Acme Corp.", user_id="alice")

# Later — the agent recalls the information
output = agent.run("Where do I work?", user_id="alice")
print(output.content)  # "You work at Acme Corp."
```

## Stream Responses

Stream tokens from an agent as they are generated:

```python
from definable.agent import Agent
from definable.model.openai import OpenAIChat

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    instructions="You are a helpful assistant.",
)

for event in agent.run_stream("Tell me a story."):
    if event.event == "RunContent" and event.content:
        print(event.content, end="", flush=True)
```

## What's Next?

<CardGroup cols={2}>
  <Card title="Models" icon="microchip" href="/models/overview">
    Explore all supported LLM providers and configurations.
  </Card>
  <Card title="Agents" icon="robot" href="/agents/overview">
    Learn about the agent lifecycle, middleware, and tracing.
  </Card>
  <Card title="Tools" icon="wrench" href="/tools/overview">
    Define custom tools with hooks, caching, and dependencies.
  </Card>
  <Card title="Knowledge" icon="book" href="/knowledge/overview">
    Build complete RAG pipelines with readers, chunkers, and vector databases.
  </Card>
  <Card title="Memory" icon="brain" href="/memory/overview">
    LLM-driven persistent memory with automatic recall and 3 storage backends.
  </Card>
  <Card title="Interfaces" icon="message-bot" href="/interfaces/overview">
    Deploy agents to Telegram, Discord, and Signal.
  </Card>
</CardGroup>
